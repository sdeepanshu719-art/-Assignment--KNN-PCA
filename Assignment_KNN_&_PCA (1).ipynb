{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "THEORY QUESTION"
      ],
      "metadata": {
        "id": "1491e_1_wxz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q- 1 What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "\n",
        "  ANS-- K-Nearest Neighbors (KNN) is a simple, intuitive, supervised machine learning algorithm used for both classification and regression tasks. It makes predictions based on the idea that similar data points exist close to each other in feature space.\n",
        "\n",
        "   how knn work: KNN is a lazy learning and instance-based algorithm, meaning it does not build a model during training. Instead, it stores the training data and makes predictions only when a query point needs to be classified or predicted.\n",
        "            \n",
        "            1. Choose a value for K (number of neighbors).\n",
        "\n",
        "            2. Compute the distance (often Euclidean) between the query point and all points in the training dataset.\n",
        "\n",
        "            3. Select the K closest points.\n",
        "\n",
        "          "
      ],
      "metadata": {
        "id": "mgeFkjPDw7ZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q- 2  What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "  ANS-- The Curse of Dimensionality refers to a set of problems that arise when data has many features (high dimensionality). As the number of dimensions grows, data becomes sparse and distance measures become less meaningful—this directly harms algorithms like K-Nearest Neighbors (KNN), which rely heavily on distance calculations.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "h_ggrr71x90E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q -3 What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "   ANS-- Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a high-dimensional dataset into a smaller set of new variables (called principal components) while retaining as much variance (information) as possible.\n",
        "\n",
        "It is one of the most widely used unsupervised learning methods for preprocessing, visualization, noise reduction, and compressing data.\n",
        "\n",
        "\n",
        "   HOW PCA Is Different from Feature Selection:-\n",
        "\n",
        "   | Aspect                   | PCA                                                  | Feature Selection                                      |\n",
        "| ------------------------ | ---------------------------------------------------- | ------------------------------------------------------ |\n",
        "| **Type**                 | Feature **extraction** / transformation              | Feature **selection**                                  |\n",
        "| **Original features?**   | Creates **new features** (linear combinations)       | Keeps **original features**                            |\n",
        "| **Interpretability**     | Lower (components are combinations of many features) | High (selected features are original variables)        |\n",
        "| **Goal**                 | Capture maximum variance                             | Keep most important original features                  |\n",
        "| **Supervision**          | Usually **unsupervised**                             | Can be supervised or unsupervised                      |\n",
        "| **Correlation handling** | Removes correlation (makes components orthogonal)    | May keep correlated features unless explicitly removed |\n",
        "  "
      ],
      "metadata": {
        "id": "-hUW2giZyxYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q- 4  What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "   ANS-- In Principal Component Analysis (PCA), eigenvalues and eigenvectors are fundamental mathematical concepts that determine the directions of the new feature space (principal components) and how much information (variance) each direction captures.\n",
        "\n",
        "   | Concept              | Meaning                              | Role in PCA                        |\n",
        "| -------------------- | ------------------------------------ | ---------------------------------- |\n",
        "| **Eigenvector**      | Direction of maximum variance        | Defines principal components       |\n",
        "| **Eigenvalue**       | Amount of variance in that direction | Ranks the importance of components |\n",
        "| **Large eigenvalue** | Component keeps lots of information  | Should be retained                 |\n",
        "| **Small eigenvalue** | Component contributes little         | Can be removed                     |\n"
      ],
      "metadata": {
        "id": "o1DgTMC1zdui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q- 5  How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "   ANS-- K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA) are often used together in a machine-learning pipeline because they solve each other’s weaknesses and improve overall performance—especially when dealing with high-dimensional data."
      ],
      "metadata": {
        "id": "7wbTwSESz4gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-6  Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "cPjt-4f40Ucn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# KNN WITHOUT feature scaling\n",
        "# -----------------------------\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN WITH feature scaling\n",
        "# -----------------------------\n",
        "knn_with_scaling = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_with_scaling.fit(X_train, y_train)\n",
        "y_pred_scaling = knn_with_scaling.predict(X_test)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE8amjMFJvMj",
        "outputId": "b42dabf2-818c-404c-eae6-396dec03ea07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7777777777777778\n",
            "Accuracy with scaling: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-7 Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component."
      ],
      "metadata": {
        "id": "IZKwzffNKFi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Wine dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Standardize features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA (keep all components)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_, start=1):\n",
        "    print(f\"Principal Component {i}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3yelDFZKNC_",
        "outputId": "6d1a0d79-6ae5-43d9-b96d-f8d5fbf5e4cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-8  Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "Jf5El0ioKaIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# KNN on ORIGINAL data (with scaling)\n",
        "# -------------------------------------------------\n",
        "knn_original = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_original.fit(X_train, y_train)\n",
        "y_pred_original = knn_original.predict(X_test)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# KNN on PCA-reduced data (top 2 components)\n",
        "# -------------------------------------------------\n",
        "knn_pca = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=2)),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "knn_pca.fit(X_train, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy on original dataset:\", accuracy_original)\n",
        "print(\"Accuracy on PCA (2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTgPPUJ2Kf4m",
        "outputId": "878fba55-aae8-4969-f5a5-dc677fb256cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset: 0.9333333333333333\n",
            "Accuracy on PCA (2 components): 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-9 Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "aKpMux3OKtsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Euclidean distance\n",
        "# -----------------------------\n",
        "knn_euclidean = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        metric=\"euclidean\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Manhattan distance\n",
        "# -----------------------------\n",
        "knn_manhattan = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        metric=\"manhattan\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"Accuracy (Euclidean):\", accuracy_euclidean)\n",
        "print(\"Accuracy (Manhattan):\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYZNycmcKze-",
        "outputId": "4fcb60ad-a00d-4011-c523-8cf63bfb9090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Euclidean): 0.9333333333333333\n",
            "Accuracy (Manhattan): 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-10 You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data"
      ],
      "metadata": {
        "id": "6luZ7mhBLDWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Simulate high-dimensional gene expression data\n",
        "X, y = make_classification(\n",
        "    n_samples=120,\n",
        "    n_features=5000,\n",
        "    n_informative=50,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 1. Linear SVM on raw data (overfitting)\n",
        "svm_raw = SVC(kernel=\"linear\")\n",
        "svm_raw.fit(X_train, y_train)\n",
        "\n",
        "train_acc_raw = accuracy_score(y_train, svm_raw.predict(X_train))\n",
        "test_acc_raw = accuracy_score(y_test, svm_raw.predict(X_test))\n",
        "\n",
        "# 2. PCA + Linear SVM\n",
        "pca = PCA(n_components=50)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "svm_pca = SVC(kernel=\"linear\")\n",
        "svm_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "train_acc_pca = accuracy_score(y_train, svm_pca.predict(X_train_pca))\n",
        "test_acc_pca = accuracy_score(y_test, svm_pca.predict(X_test_pca))\n",
        "\n",
        "print(\"Raw SVM - Train Accuracy:\", train_acc_raw)\n",
        "print(\"Raw SVM - Test Accuracy:\", test_acc_raw)\n",
        "print(\"PCA + SVM - Train Accuracy:\", train_acc_pca)\n",
        "print(\"PCA + SVM - Test Accuracy:\", test_acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG-2Ny3jm9HN",
        "outputId": "61814730-571e-4981-8438-d60c68016a54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw SVM - Train Accuracy: 1.0\n",
            "Raw SVM - Test Accuracy: 0.5\n",
            "PCA + SVM - Train Accuracy: 1.0\n",
            "PCA + SVM - Test Accuracy: 0.4722222222222222\n"
          ]
        }
      ]
    }
  ]
}